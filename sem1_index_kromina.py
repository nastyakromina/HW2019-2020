# -*- coding: utf-8 -*-
"""sem1_Index_Kromina.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SALhacbckwJ9eFttMkFTweToSqAqpY6u

## Семинар 1 Индекс

## Intro

### работа с файлами и папками
"""

import os

curr_dir = os.getcwd()
filepath = os.path.join(curr_dir, 'test.txt')

"""### os.path  
путь до файла
"""

# возвращает полный путь до папки/файла по имени файла / папки
print(os.path.abspath(filepath))


# возвращает имя файла / папки по полному ти до него
print(os.path.basename(filepath))


# проверить существование директории - True / False
print(os.path.exists(curr_dir))

"""### os.listdir  
возвращает список файлов в данной директории
"""

os.listdir(curr_dir)

"""При обходе файлов не забывайте исключать системные директории, такие как .DS_Store

### os.walk
root - начальная директория  
dirs - список поддиректорий (папок)   
files - список файлов в этих поддиректориях
"""

for root, dirs, files in os.walk(curr_dir):
    for name in files:
        print(os.path.join(root, name))

"""> __os.walk__ возвращает генератор, это значит, что получить его элементы можно только проитерировавшись по нему  
но его легко можно превратить в list и увидеть все его значения
"""

list(os.walk(curr_dir))

"""### чтение файла"""

filepath = 'test.txt'


# одним массивом  
with open(filepath, 'r') as f:  
    text = f.read() 

    
#по строкам, в конце каждой строки \n  
#with open(fpath, 'r') as f:   
#    text = f.readlines() 

    
#по строкам, без \n   
#with open(fpath, 'r') as f:   
#    text = f.read().splitlines()

"""Напоминание про enumerate:    
> При итерации по списку вы можете помимо самого элемента получить его порядковый номер    
``` for i, element in enumerate(your_list): ...  ```    
Иногда для получения элемента делают так -  ``` your_list[i] ```, не надо так

##  Индекс 

Сам по себе индекс - это просто формат хранения данных, он не может осуществлять поиск. Для этого необходимо добавить к нему определенную метрику. Это может быть что-то простое типа булева поиска, а может быть что-то более специфическое или кастомное под задачу.

Давайте посмотрим, что полезного можно вытащить из самого индекса.    
По сути, индекс - это информация о частоте встречаемости слова в каждом документе.   
Из этого можно понять, например:
1. какое слово является самым часто употребимым / редким
2. какие слова встречаются всегда вместе - так можно парсить твиттер, fb, форумы и отлавливать новые устойчивые выражения в речи
3. как эти документы кластеризуются по N тематикам согласно словам, которые в них упоминаются

## __Задача__: 

**Data:** Коллекция субтитров сезонов Друзьей. Одна серия - один документ.

**To do:** Постройте небольшой модуль поискового движка, который сможет осуществлять поиск по коллекции документов.
На входе запрос и проиндексированная коллекция (в том виде, как посчитаете нужным), на выходе отсортированный по релевантности с запросом список документов коллекции. 

Релизуйте:
    - функцию препроцессинга данных
    - функцию индексирования данных
    - функцию метрики релевантности 
    - собственно, функцию поиска

[download_friends_corpus](https://yadi.sk/d/yVO1QV98CDibpw)

Напоминание про defaultdict: 
> В качестве multiple values словаря рекомендую использовать ``` collections.defaultdict ```                          
> Так можно избежать конструкции ``` dict.setdefault(key, default=None) ```
"""

### _check : в коллекции должно быть около 165 файлов

pip install pymorphy2

"""С помощью обратного индекса посчитайте:  


a) какое слово является самым частотным

b) какое самым редким

c) какой набор слов есть во всех документах коллекции

d) какой сезон был самым популярным у Чендлера? у Моники?

e) кто из главных героев статистически самый популярный?
"""

import os
import re
import pymorphy2
morph = pymorphy2.MorphAnalyzer()

from google.colab import drive
drive.mount('/content/gdrive')

os.chdir('gdrive/My Drive/Colab Notebooks/friends')

for file in os.listdir(os.getcwd()):
  print(file)

top = os.getcwd()
top

folder = []
for i in os.walk(top):
  folder.append(i)

allTexts = []
allNames = []
for address, dirs, files in folder:
  for file in files:
    allNames.append(file)
    with open(address+'/'+file, 'r') as f:
      text = f.read()
      allTexts.append(text)

len(allTexts)

allWordsAllTexts = []
for text in allTexts:
  allWordsInOneText = []
  if text != "":
    for word in text.split():
      word = re.sub('[.,-;:?!@#$%^&()_+=—\ufeff–"…«»>wwwtvsubtitlesnet]', '', word).lower()
      if word != "":
        p = morph.parse(word)[0]
        if p not in allWordsInOneText:
          allWordsInOneText.append(p.normal_form)
    allWordsAllTexts.append(allWordsInOneText)

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

vectorizer = CountVectorizer()

arrayString = []
for text in allWordsAllTexts:
  s = ' '.join(text)
  arrayString.append(s)

X = vectorizer.fit_transform(arrayString)

Matrix = X.toarray()

print(Matrix)

import pandas as pd
df = pd.DataFrame(Matrix, index=allNames, columns=vectorizer.get_feature_names())

df.head()

"""**Поисковик**"""

from collections import Counter
import operator
import math

def tokenize(doc):
  allWordsInOneText = []
  text = doc.split()
  for word in text:
    word = re.sub('[.,-;:?!@#$%^&()_+=—\ufeff–"…«»>wwwtvsubtitlesnet]', '', word).lower()
    if word != "":
      p = morph.parse(word)[0]
      allWordsInOneText.append(p.normal_form)
  return allWordsInOneText

def build_terms(corpus):
    terms = {}
    current_index = 0
    for doc in corpus:
        for word in tokenize(doc):
            if word not in terms:
                terms[word] = current_index
                current_index += 1
    return terms

def tf(document, terms):
    words = tokenize(document)
    total_words = len(words)
    doc_counter = Counter(words)
    for word in doc_counter:
        doc_counter[word] /= total_words
    tfs = [0 for _ in range(len(terms))]
    for term, index in terms.items():
        tfs[index] = doc_counter[term]
    return tfs

def _count_docs_with_word(word, docs):
    counter = 1
    for doc in docs:
        if word in doc:
            counter += 1
    return counter

# documents - это корпус
def idf(documents, terms):
    idfs = [0 for _ in range(len(terms))]
    total_docs = len(documents)
    for word, index in terms.items():
        docs_with_word = _count_docs_with_word(word, documents)
        idf = 1 + math.log10(total_docs / docs_with_word)
        idfs[index] = idf
    return idfs

def _merge_td_idf(tf, idf, terms):
    return [tf[i] * idf[i] for i in range(len(terms))]


def build_tfidf(corpus, document, terms):
    doc_tf = tf(document, terms)
    doc_idf = idf(corpus, terms)
    return _merge_td_idf(doc_tf, doc_idf, terms)


def cosine_similarity(vec1, vec2):
    def dot_product2(v1, v2):
        return sum(map(operator.mul, v1, v2))

    def vector_cos5(v1, v2):
        prod = dot_product2(v1, v2)
        len1 = math.sqrt(dot_product2(v1, v1))
        len2 = math.sqrt(dot_product2(v2, v2))
        return prod / (len1 * len2)

    return vector_cos5(vec1, vec2)

tf_idf_total = []
corpus = allTexts
terms = build_terms(corpus)

len(corpus)

for document in corpus:
  tf_idf_total.append(build_tfidf(corpus, document, terms))

query = "Он подарил мне цветы"

results = {}
query_tfidf = build_tfidf(corpus, query, terms)
for index, document in enumerate(tf_idf_total):
  results[index] = cosine_similarity(query_tfidf, document)

ld = list(results.items())
ld.sort(key=lambda i: i[1], reverse= True)
ld

print('По вашему запросу лучший результат - ' + allNames[ld[0][0]])